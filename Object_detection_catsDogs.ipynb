{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CsNovl0NtyH",
    "outputId": "07a5d500-ac66-4c45-eefa-63f92a6ed6a5"
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsqgUWOD38vl"
   },
   "source": [
    "**0) Setup and Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ODebk1t36IN",
    "outputId": "6794b42f-1780-4b20-9c4b-bcf63c17c744"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CHjZP1c4ElG"
   },
   "source": [
    "**1) Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfGEyYrO4D-e"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '.'\n",
    "\n",
    "# Define paths for the original dataset and the new structured dataset\n",
    "DATASET_SOURCE_PATH = os.path.join(BASE_PATH, 'dataset') # Train and Test folders are here\n",
    "DATASET_DEST_PATH = os.path.join(BASE_PATH, 'yolo_dataset') # Structured dataset will be created here\n",
    "\n",
    "# Define paths for the new structure\n",
    "TRAIN_IMG_PATH = os.path.join(DATASET_DEST_PATH, 'images/train')\n",
    "VAL_IMG_PATH = os.path.join(DATASET_DEST_PATH, 'images/val')\n",
    "TRAIN_LBL_PATH = os.path.join(DATASET_DEST_PATH, 'labels/train')\n",
    "VAL_LBL_PATH = os.path.join(DATASET_DEST_PATH, 'labels/val')\n",
    "\n",
    "YAML_PATH = os.path.join(BASE_PATH, 'data.yaml')\n",
    "PROJECT_DIR = BASE_PATH # Directory to save training results\n",
    "\n",
    "\n",
    "# Create necessary directories\n",
    "for path in [TRAIN_IMG_PATH, VAL_IMG_PATH, TRAIN_LBL_PATH, VAL_LBL_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define class mapping\n",
    "CLASSES = {'dog': 0, 'cat': 1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8VJJIyO5qt-"
   },
   "source": [
    "# PART 1: Object Detection Framework Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ-OuHCR5zH7"
   },
   "source": [
    "**Step 1: Dataset Prepareation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYlejKel5xie"
   },
   "outputs": [],
   "source": [
    "def parse_xml_to_df(xml_files):\n",
    "    \"\"\"Parses a list of XML files and returns a pandas DataFrame.\"\"\"\n",
    "    xml_data = []\n",
    "    for filename in xml_files:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        image_name = root.find('filename').text\n",
    "        img_width = int(root.find('size').find('width').text)\n",
    "        img_height = int(root.find('size').find('height').text)\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "            xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "            ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "            ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "            xml_data.append([image_name, img_width, img_height, class_name, xmin, xmax, ymin, ymax])\n",
    "\n",
    "    return pd.DataFrame(xml_data, columns=['filename', 'width', 'height', 'name', 'xmin', 'xmax', 'ymin', 'ymax'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkIJR_0e4CP6"
   },
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(df):\n",
    "    \"\"\"Converts bounding box coordinates to YOLO format and adds class IDs.\"\"\"\n",
    "    df['center_x'] = ((df['xmin'] + df['xmax']) / 2) / df['width']\n",
    "    df['center_y'] = ((df['ymin'] + df['ymax']) / 2) / df['height']\n",
    "    df['w'] = (df['xmax'] - df['xmin']) / df['width']\n",
    "    df['h'] = (df['ymax'] - df['ymin']) / df['height']\n",
    "\n",
    "    # Label encode the class names\n",
    "    df['id'] = df['name'].map(CLASSES)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DzPwDBd6FgY"
   },
   "outputs": [],
   "source": [
    "def save_yolo_labels(df, labels_path):\n",
    "    \"\"\"Saves the DataFrame annotations to YOLO .txt format.\"\"\"\n",
    "    for filename, group in df.groupby('filename'):\n",
    "        txt_filename = os.path.join(labels_path, os.path.splitext(filename)[0] + '.txt')\n",
    "        group[['id', 'center_x', 'center_y', 'w', 'h']].to_csv(\n",
    "            txt_filename, sep=' ', index=False, header=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOaQf9e56U0k"
   },
   "source": [
    "**Main Data Preparation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49kpUwvx6Hic",
    "outputId": "396951e6-8c6b-4f83-fe48-4deaa8f1ead7"
   },
   "outputs": [],
   "source": [
    "# 1. Find all XML files\n",
    "all_xml_files = glob(os.path.join(DATASET_SOURCE_PATH, '*/*.xml'), recursive=True)\n",
    "\n",
    "# 2. Parse all XMLs into a single DataFrame\n",
    "full_df = parse_xml_to_df(all_xml_files)\n",
    "print(f\"Found and parsed {len(full_df)} annotations from {full_df['filename'].nunique()} images.\")\n",
    "\n",
    "# 3. Get a list of unique image files\n",
    "unique_images = full_df['filename'].unique()\n",
    "\n",
    "# 4. Split files into training and validation sets (e.g., 80% train, 20% val)\n",
    "# This is a more robust way to split than using pre-defined folders\n",
    "train_files, val_files = train_test_split(unique_images, test_size=0.2, random_state=42)\n",
    "print(f\"Splitting into {len(train_files)} training images and {len(val_files)} validation images.\")\n",
    "\n",
    "# 5. Create train and validation DataFrames\n",
    "train_df = full_df[full_df['filename'].isin(train_files)]\n",
    "val_df = full_df[full_df['filename'].isin(val_files)]\n",
    "\n",
    "# 6. Convert to YOLO format\n",
    "train_df = convert_to_yolo_format(train_df)\n",
    "val_df = convert_to_yolo_format(val_df)\n",
    "\n",
    "# 7. Save YOLO label files\n",
    "save_yolo_labels(train_df, TRAIN_LBL_PATH)\n",
    "save_yolo_labels(val_df, VAL_LBL_PATH)\n",
    "print(\"Saved YOLO format labels for train and validation sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD7cjbUZ6fE0",
    "outputId": "84097f32-0e74-4cc0-d256-268d72f671ad"
   },
   "outputs": [],
   "source": [
    "# 8. Copy image files to the new structured directories\n",
    "def copy_images(filenames, source_dir, dest_dir):\n",
    "    for filename in filenames:\n",
    "        # Assuming images are in the same folder as their xmls (e.g., 'dataset/train/')\n",
    "        source_img_path = os.path.join(source_dir, os.path.splitext(filename)[0].split('/')[0], filename)\n",
    "        # Handle cases where source images might be in 'train' or 'test' folders\n",
    "        train_path = os.path.join(DATASET_SOURCE_PATH, 'train', filename)\n",
    "        test_path = os.path.join(DATASET_SOURCE_PATH, 'test', filename)\n",
    "        if os.path.exists(train_path):\n",
    "             shutil.copy(train_path, os.path.join(dest_dir, filename))\n",
    "        elif os.path.exists(test_path):\n",
    "             shutil.copy(test_path, os.path.join(dest_dir, filename))\n",
    "\n",
    "copy_images(train_files, DATASET_SOURCE_PATH, TRAIN_IMG_PATH)\n",
    "copy_images(val_files, DATASET_SOURCE_PATH, VAL_IMG_PATH)\n",
    "print(\"Copied images to new train/val directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL1DeGpo_Q-w",
    "outputId": "24471b2b-34b2-485d-bdfe-f531b8be4b18"
   },
   "outputs": [],
   "source": [
    "# 9. Create the data.yaml file programmatically\n",
    "yaml_content = {\n",
    "    'train': os.path.abspath(TRAIN_IMG_PATH),\n",
    "    'val': os.path.abspath(VAL_IMG_PATH),\n",
    "    'nc': len(CLASSES),\n",
    "    'names': list(CLASSES.keys())\n",
    "}\n",
    "\n",
    "with open(YAML_PATH, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Generated data.yaml file at: {YAML_PATH}\")\n",
    "print(\"--- Dataset Preparation Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKPXmLe7_t30"
   },
   "source": [
    "**Step 2: Model Development & Experimentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X35DgFUhBlC0",
    "outputId": "5699af9f-fa50-45d0-841d-c68d963d59b3"
   },
   "outputs": [],
   "source": [
    "# Set the working directory to save results properly\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Changed working directory to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldrMoL9ECjlf",
    "outputId": "029d4159-bae3-425a-abd1-19e2c1caadf3"
   },
   "outputs": [],
   "source": [
    "# --- Experiment 1: Training YOLOv8s, with 30 epochs, 8 batches, and 640 image size - This is the baseline ---\n",
    "print(\"\\n--- Starting Architecture Experiment 1: Training YOLOv8s, with 30 epochs, 8 batches, and 640 image size ---\")\n",
    "model_small = YOLO('yolov8s.pt')\n",
    "results_small = model_small.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=30,\n",
    "    batch=8,\n",
    "    imgsz=640,\n",
    "    name='yolov8s_architecture_exp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYODetXtCtCv",
    "outputId": "36760e01-4565-48b9-8ff3-9928d9ce9a20"
   },
   "outputs": [],
   "source": [
    "#------Experiment 2 - Experimenting with increased no of epochs (50) ----\n",
    "# This experiment helps to understand how performance changes with more training time. It helps identify if the model is undertrained or if it starts to overfit.\n",
    "# Baseline is 30 epochs from the previous experiment.\n",
    "\n",
    "print(\"\\n--- Starting Parameter Experiment: Training for 50 Epochs ---\")\n",
    "model_s = YOLO('yolov8s.pt')\n",
    "model_s.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=50, # Increased epochs\n",
    "    batch=8,\n",
    "    imgsz=640,\n",
    "    name='yolov8s_50_epochs_exp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI4tiiJXAaha",
    "outputId": "f7c292db-120c-411e-c548-b6fa77f97dd9"
   },
   "outputs": [],
   "source": [
    "# Experiement 3 - Experimenting with changed Image Size (320)  ---\n",
    "# This experiement is to understand the impact of input resolution on detection performance, especially for small objects.\n",
    "# Baseline is imgsz=640\n",
    "print(\"\\n--- Starting Parameter Experiment: Training with imgsz=320 ---\")\n",
    "model_s = YOLO('yolov8s.pt')\n",
    "model_s.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=30,\n",
    "    batch=8,\n",
    "    imgsz=320, # Reduced image size\n",
    "    name='yolov8s_imgsz320_exp'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EOtZ8kRIXWF",
    "outputId": "3610a259-01ac-4d72-d0db-189dc67becfb"
   },
   "outputs": [],
   "source": [
    "# --- Experiment 4: Experimenting with model size (architecture) ---\n",
    "#different YOLOv8 model sizes (n, s, m, l, x) are not just scaled-up versions of each other;\n",
    "#they have different numbers of layers and channels in their backbone (the feature extractor) and neck.\n",
    "\n",
    "print(\"\\n--- Starting Architecture Experiment : Training YOLOv8n ---\")\n",
    "model_nano = YOLO('yolov8n.pt')\n",
    "results_nano = model_nano.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=30,\n",
    "    batch=8,\n",
    "    imgsz=640,\n",
    "    name='yolov8n_architecture_exp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7UfK8hN88KQ"
   },
   "source": [
    "**Step 3: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OU1iHN621z21",
    "outputId": "74c4d7fd-6386-4e8e-80c7-b39721ebcdb0"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def eval_run(run_name, imgsz=IMGSZ_EVAL):\n",
    "    w = Path(PROJECT_DIR) / 'runs' / 'detect' / run_name / 'weights' / 'best.pt'\n",
    "    assert w.exists(), f\"Best weights not found for {run_name}\"\n",
    "    m = YOLO(str(w))\n",
    "    eval_name = f\"{run_name}_evaluation_bycode\"  # Custom eval folder name\n",
    "    vr = m.val(\n",
    "        data=YAML_PATH,\n",
    "        imgsz=imgsz,\n",
    "        split='val',\n",
    "        plots=True,\n",
    "        save_json=True,\n",
    "        name=eval_name,             # Custom folder for this evaluation\n",
    "        project='runs/detect'       # Keep all evals under runs/detect\n",
    "    )\n",
    "    d = vr.results_dict\n",
    "    return {\n",
    "        'run': run_name,\n",
    "        'eval_folder': f\"runs/detect/{eval_name}\",   # <- store the eval folder for reference!\n",
    "        'imgsz_eval': imgsz,\n",
    "        'mAP@0.5': float(d.get('metrics/mAP50', d.get('metrics/mAP50(B)', 0.0))),\n",
    "        'mAP@0.5:0.95': float(d.get('metrics/mAP50-95', d.get('metrics/mAP50-95(B)', 0.0))),\n",
    "        'Precision': float(d.get('metrics/precision', d.get('metrics/precision(B)', 0.0))),\n",
    "        'Recall': float(d.get('metrics/recall', d.get('metrics/recall(B)', 0.0))),\n",
    "    }\n",
    "\n",
    "summary = [eval_run(r) for r in RUNS]\n",
    "eval_df = pd.DataFrame(summary).sort_values('mAP@0.5:0.95', ascending=False)\n",
    "display(eval_df)  # nice table in notebook\n",
    "\n",
    "# Save a CSV for your report appendix\n",
    "eval_csv = os.path.join(PROJECT_DIR, 'evaluation_summary.csv')\n",
    "eval_df.to_csv(eval_csv, index=False)\n",
    "print(\"Saved:\", eval_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZS97HnmByTV"
   },
   "source": [
    "**Step 4: Result of Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PvlhQ2Fl9VAC",
    "outputId": "097033c7-bdfe-43dc-ddb9-fc9c0be929fb"
   },
   "outputs": [],
   "source": [
    "# Set the parameters\n",
    "BEST_RUN = 'yolov8s_50_epochs_exp'  # <-- set the run to be visualized based on the evaluation earlier\n",
    "EVAL_FOLDER = f\"{BEST_RUN}_evaluation_bycode\"\n",
    "WEIGHTS_PATH = Path(PROJECT_DIR) / 'runs' / 'detect' / BEST_RUN / 'weights' / 'best.pt'\n",
    "VISUALIZE_DIR = Path(PROJECT_DIR) / 'visualisations' / BEST_RUN\n",
    "\n",
    "os.makedirs(VISUALIZE_DIR, exist_ok=True)\n",
    "\n",
    "# Load best model\n",
    "model = YOLO(str(WEIGHTS_PATH))\n",
    "\n",
    "# Pick sample validation images\n",
    "val_images = sorted(\n",
    "    glob.glob(os.path.join(VAL_IMG_PATH, '*.jpg')) +\n",
    "    glob.glob(os.path.join(VAL_IMG_PATH, '*.jpeg')) +\n",
    "    glob.glob(os.path.join(VAL_IMG_PATH, '*.png'))\n",
    ")\n",
    "SAMPLE_SIZE = 16\n",
    "sample_imgs = val_images[:SAMPLE_SIZE]\n",
    "\n",
    "# Run prediction and save images\n",
    "pred_results = model.predict(\n",
    "    sample_imgs,\n",
    "    imgsz=640,\n",
    "    conf=0.25,\n",
    "    save=True,\n",
    "    project=str(VISUALIZE_DIR),\n",
    "    name='viz_preds'\n",
    ")\n",
    "print(f\"Saved predictions to: {VISUALIZE_DIR/'viz_preds'}\")\n",
    "\n",
    "# Show a few good predictions\n",
    "def show_image_with_boxes(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n-- Example Good Predictions --\")\n",
    "pred_img_dir = VISUALIZE_DIR / 'viz_preds'\n",
    "all_pred_imgs = sorted(glob.glob(str(pred_img_dir / '*.jpg')))\n",
    "for img_path in all_pred_imgs[:4]:  # Show first 4 (likely to be correct)\n",
    "    show_image_with_boxes(img_path)\n",
    "\n",
    "# Find and show wrongly predicted (weak/failure) cases\n",
    "weak_cases = []\n",
    "for res, img_path in zip(pred_results, sample_imgs):\n",
    "    # 1) Missed: no boxes predicted\n",
    "    if len(res.boxes) == 0:\n",
    "        weak_cases.append((\"NO DETECTION\", img_path, 0.0))\n",
    "    else:\n",
    "        # 2) Weak: lowest confidence in boxes\n",
    "        conf = float(res.boxes.conf.min().cpu())\n",
    "        # 3) Identify wrong class by comparing with ground truth\n",
    "        weak_cases.append((\"LOW CONF\", img_path, conf))\n",
    "\n",
    "# Sort: missed first, then lowest confidence\n",
    "weak_cases_sorted = sorted(weak_cases, key=lambda x: x[2])[:6]\n",
    "\n",
    "print(\"\\n-- Example Weak/Failure Cases --\")\n",
    "for tag, img_path, conf in weak_cases_sorted:\n",
    "    print(f\"{tag} | {os.path.basename(img_path)} | conf: {conf:.3f}\")\n",
    "    show_image_with_boxes(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Olq-uMjWPfRM"
   },
   "source": [
    "# PART 2: Real Time Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8G1z_FfPst_"
   },
   "source": [
    "**Step 1: Load the Best model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bp33AASKNML5"
   },
   "outputs": [],
   "source": [
    "# Point to the best model's weights\n",
    "BEST_RUN = 'yolov8s_50_epochs_exp'\n",
    "WEIGHTS_PATH = Path(PROJECT_DIR) / 'runs' / 'detect' / BEST_RUN / 'weights' / 'best.pt'\n",
    "\n",
    "model = YOLO(str(WEIGHTS_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSPoA6ltQDsb"
   },
   "source": [
    "**Step 2: Run Inference on the Video**\n",
    "\n",
    "YOLOv8 makes this very simple. It will:\n",
    "\n",
    "\n",
    "*   Process each frame of the video\n",
    "*   Draw bounding boxes and class labels\n",
    "*   Save the output as a new video file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6aTrOaQP3UZ",
    "outputId": "0d4ce50d-0ba1-4202-84cb-078f591d81ea"
   },
   "outputs": [],
   "source": [
    "# Input and output\n",
    "video_input_path = 'catDog.mp4' # Example video file name\n",
    "if os.path.exists(video_input_path):\n",
    "    print(f\"Running inference on {video_input_path}...\")\n",
    "    model.predict(\n",
    "        source=video_input_path,\n",
    "        conf=0.25,\n",
    "        save=True,\n",
    "        project='runs',\n",
    "        name='video_inference'\n",
    "    )\n",
    "    print(\"Video inference complete. Output saved in 'runs/video_inference'.\")\n",
    "else:\n",
    "    print(f\"Video file not found at {video_input_path}. Skipping video inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
